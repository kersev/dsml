# Week 6 - Exercise

# **Machine learning using the Caret Package & Introduction to Synthetic Data**

The caret package is a powerful machine learning package in R.

<https://topepo.github.io/caret/>

Open R studio and activate the caret package:

```{r}
library("caret")
```

Download the cancer dataset as a CSV file and import into R studio:

<https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data>

Select all rows and the first 32 columns:

```{r}
data <- data[,c(1:32)]
```

Convert the data\$diagnosis variable to a factor().

```{r}
data$diagnosis <- as.factor(data$diagnosis)
```

Use createDataPartition function to create a training and test set (70/30
split).

```{r}
partition <- createDataPartition(data$diagnosis, p=0.7, list=FALSE)
trainingData <- data[partition,]
testData <- data[-partition,]

```

You can learn more about the createDataPartition() function using the
documentation:

```{r}
?createDataPartition
```

or on the web page:

<https://topepo.github.io/caret/data-splitting.html>

e.g. the argument p is the proportion for the training set.

### Now build a model using K-NN with caret using the training set:

```{r}
model <- train(diagnosis~., method="knn", data= trainingData)
```

The diagnosis column is the label and here we are using all other columns as the
features.

Type model in the console and you should get the following output:

```{r}
model
```

You can see the accuracy and kappa for different K-NN models using different
values for the parameter k. You can see that the model performs best when using
5 nearest neighbours when using the training data for classification.

Now test the model on the test set:

```{r}
testingTheModel <- predict(model, newdata = testData)
```

Use the confusion matrix function to study the various evaluation metrics:

```{r}
confusionMatrix(testingTheModel, testData$diagnosis)
```

Study the metrics. Kappa is a measure of agreement between the predicted and the
actual labels in the test set.

The no-information rate is the proportion of the data with the majority class.

The P-Value [Acc \> NIR] is a statistical test that tests to see if there is a
statistically significant difference between the accuracy of the algorithm
versus the no information rate. For a model to be useful, the algorithm would
typically need a higher accuracy than the no information rate. The no
information rate on a balanced test set in a binary classification problem would
be 50%.

The following code will also tell use what the no-information rate is:

```{r}
table(testData$diagnosis)
```

So B is the majority class, hence the no information rate is 107/170 (\~62.94%).

Try the following statistical test to get a p-value when comparing proportion of
correctly classified cases (accuracy) vs. the no-information rate:

```{r}
prop.test(c(116,107),c(170,170))
```

You can see that the accuracy is not statistically significantly better than the
no-information rate.

You can use the following code to setup 10-fold cross validation:

```{r}
control <- trainControl(method="repeatedcv", number=10, savePredictions = TRUE)
model <- train(diagnosis~., method="knn", data= trainingData, trControl=control)
model
```

Even with 10-fold CV, a model where is k=5 is still the best K-NN classifier.

You can see the predictions for each fold using the following code:

```{r}
cv<-model$pred
cv
```

You can see that there were 10-folds as expected:

```{r}
table(model$pred$Resample)
```

Obviously 399 (size of training set) divided by 10 is almost \~40 samples per
fold, however you will notice that 3 different K-NN algorithms were also tested
(with k as 5, 7 and 9 respectively).

As you could see, the method argument in the train function specifies the
machine learning algorithm to be used.

Use the following code to see the range of machine learning algorithms that
could be used:

```{r}
names(getModelInfo())
```

Now try a different technique such as support vector machines:

```{r}
model <- train(diagnosis~., method="svmLinear", data= trainingData, trControl=control)
model
```

You can see that the SVM perform better than the K-NN model.

This time you will see that there are around 40 cases per fold:

```{r}
cv<-model$pred
table(model$pred$Resample)
```

As a useful exercise, consider building a range of models and present the
results (accuracy, sensitivity etc.) from each model on a useful graph to show
how each model performs in comparison to each other.

# **Synthetic data**

When using a dataset with class imbalance, the dataset can be balanced by
under-sampling and/or oversampling. You can increase the size of the minority
class by simulating synthetic cases.

One approach is using SMOTE which stands for Synthetic Minority Over-sampling
Technique.

Install and activate the smotefamily package.

```{r}
library(smotefamily)
```

See example provided in documentation:

```{r}
?SMOTE
```

Generate a dataset:

```{r}
dataSet = sample_generator(1000, ratio = 0.70)
```

This generates a dataset with 1000 cases, 2 features and one label/class called
result.

The ratio argument defines the class imbalance (70% for the majority class) in
the dataset as can be seen below:

```{r}
table(dataSet$result)
```

Generate synthetic data for class balancing:

```{r}
generateData = SMOTE(dataSet[,c(1,2)], dataSet[,c(3)], K=5)
```

The following now stores a new dataset that includes the original dataset along
with the synthetic data in the minority class.

```{r}
newDataSet <- generateData$data
```

You can now check the new class balance which is now improved:

```{r}
table(newDataSet$class)
```

You can also create an entire synthetic dataset from an original dataset if
indeed you wanted to share data that reflects the characteristics of the real
dataset but remains synthetic / simulated.

Review the **synthpop** package:

<https://synthpop.org.uk/get-started.html>

Use the online tool to generate synthetic data from the data variables provided
and then explore the comparative plots that show the similarities and
differences between the original dataset and the corresponding synthetic
dataset.

<https://synthpop.shinyapps.io/synthpop/>

You can explore the example provided on the website and also review the paper:

<https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/20150/Paper_24_bnowok_synthpop.pdf>
